---
title: "Técnica Random Forest em árvores de decisão"
subtitle: "Grupo 4"
author:
  - name: Bruno Gondim Toledo
    url: https://github.com/penasta
  - name: Stefan Zurman Gonçalves
  - name: João Pedro Almeida Santos
  - name: João Alberto de Rezende Alvares

format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: true
    preview-links: auto
    scrollable: true
    logo: as_vert_cor.jpg
    theme: serif
    width: 1500
    css: styles.css
    footer: Departamento de estatística - UnB
resources:
  - demo.pdf
editor_options: 
  chunk_output_type: console
knitr:
  opts_chunk:
    dev: png
    dev.args:
      bg: transparent
---

```{r, include=FALSE}
if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
pacman::p_load(tidyverse)
```

## Introdução

Florestas aleatórias, ou *Random Forest*, é uma técnica em que iremos partir da ideia de árvores aleatórias (*random trees*) para fazer um classificador robusto. Em poucas palavras, poderia ser descrita como combinar força bruta computacional com seleção de modelos de árvores gerados via bootstrap por meio de uma "votação".

O procedimento do algoritmo pode ser descrito como:

::: incremental
- 1. Definir o número de árvores a serem geradas (quanto mais, melhor);
- 2. Colher uma amostra bootstrap de tamanho n dos dados de tamanho n;
- 3. Aplicar o algoritmo de árvore aleatória para cada uma das árvores geradas;
- 4. Colher os resultados das árvores e optar pelo melhor modelo (a moda entre as árvores).
:::

## Rotina

Uma rotina minimalista de classificação via *Random Forest* em **R** pode ser executada da seguinte forma:


::: columns

::: {.column width="50%"}

```{r}
#| echo: TRUE

library(randomForest)

data <- iris

data$Species <- as.factor(data$Species)

set.seed(150167636)
ind <- sample(2, nrow(data), replace = TRUE,
              prob = c(0.7, 0.3))
train <- data[ind==1,]
test <- data[ind==2,]

rf <- randomForest(Species~., data=train, proximity=TRUE)

```

:::

::: {.column width="50%"}

```{r}
rf
```

Matriz de confusão:

```{r}
#| echo: true

rf[["confusion"]]
```

:::

:::

## Resultados

Podemos acessar os resultados do modelo no objeto *rf*

::: columns

::: {.column width="50%"}

Teste do modelo no conjunto de treino:

```{r}
#| echo: true

library(caret)

p1 <- predict(rf, train)
confusionMatrix(p1, train$ Species)
```

:::

::: {.column width="50%"}

Validação do modelo nos dados de teste:

```{r}
#| echo: true

p2 <- predict(rf, test)
confusionMatrix(p2, test$ Species)

```

:::
:::

## Observações

- Vemos que o modelo em seu estado minimalista é extremamente simples de ser executado, e é uma modelagem extremamente robusta a *overfitting*, visto que irá gerar vários modelos (árvores) com dados *bootstrap* e selecionar o modelo que melhor se ajusta.

- Já está implementado na função o procedimento de tomar uma amostra *bootstrap* dos dados fornecidos para cada árvore automaticamente.

## Parâmetros

- A função *randomForest* do pacote homônimo tem uma série de parâmetros opcionais além do mínimo obrigatório, que seria o modelo e os dados. O mais importante destes parâmetros é o *ntree*, que por *default* é 500 e em geral deve-se utilizar o máximo possível tal que execute em um tempo aceitável. Em geral, o restante dos parâmetros deve ser deixado em *default*.

- Neste caso, o modelo foi extremamente eficiente mesmo na versão minimalista

## Fine tuning

Se for o caso, também podemos fazer o *fine-tuning* dos parâmetros do modelo

::: columns

::: {.column width="33%"}


```{r}
#| echo: true

t <- tuneRF(train[,-5], train[,5],
       stepFactor = 0.5,
       plot = TRUE,
       ntreeTry = 150,
       trace = TRUE,
       improve = 0.05)

```
:::
::: {.column width="33%"}
```{r}
#| echo: true

hist(treesize(rf),
     main = "No. of Nodes for the Trees",
     col = "green")

```

:::
::: {.column width="33%"}

```{r}
#| echo: true

varImpPlot(rf,
           sort = T,
           n.var = 10,
           main = "Top 10 - Variable Importance")

importance(rf)

```


:::
:::

## Importâncias

::: columns

::: {.column width="50%"}

```{r}
partialPlot(rf, train, Petal.Width, "setosa")
```


:::

::: {.column width="50%"}

```{r}
MDSplot(rf, train$Species)
```


:::

:::


## Implementações

Uma rotina de classificação via *Random forest* poderia ser executada de forma análoga da seguinte forma:

::: columns

::: {.column width="50%"}

```{r}
#| echo: true

library(reticulate)
```

```{python}
#| echo: true

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn import datasets

iris = datasets.load_iris()
dados = pd.DataFrame(data=iris.data, columns=iris.feature_names)
```

```{python}
#| echo: true

X = dados
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
```

:::

::: {.column width="50%"}

```{python}
#| echo: true

model = RandomForestClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
confusion_matrix(y_test, y_pred)
```

:::
:::

# Referências:

https://www.r-bloggers.com/2021/04/random-forest-in-r/